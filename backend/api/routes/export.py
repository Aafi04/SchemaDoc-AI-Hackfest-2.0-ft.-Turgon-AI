"""
Export API Routes — generate markdown, JSON and business-ready reports.
GET /api/export/{run_id}/json
GET /api/export/{run_id}/markdown
GET /api/export/{run_id}/report  — AI-enhanced business report (JSON)
GET /api/export/{run_id}/report/markdown — AI-enhanced business report (MD)
"""
import json
import logging
from datetime import datetime, timezone
from fastapi import APIRouter, HTTPException, Request
from fastapi.responses import Response, JSONResponse
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import SystemMessage, HumanMessage
from backend.services.pipeline_service import get_run
from backend.core.config import settings
from backend.core.utils import DecimalEncoder
from backend.core.rate_limiter import limiter, EXPORT_REPORT_LIMIT, READ_LIMIT

logger = logging.getLogger(__name__)
router = APIRouter(prefix="/api/export", tags=["Export"])


def _sid(request: Request) -> str:
    return request.headers.get("x-session-id", "")


# In-memory cache for generated business reports (avoids re-calling Gemini)
# Keyed by "{session_id}:{run_id}" for session isolation
_report_cache: dict[str, dict] = {}


def clear_session_reports(session_id: str = ""):
    """Remove cached reports for a session. If empty, clear all."""
    if not session_id:
        _report_cache.clear()
        return
    keys_to_remove = [k for k in _report_cache if k.startswith(f"{session_id}:")]
    for k in keys_to_remove:
        del _report_cache[k]


def _get_null_pct(stats, row_count=0):
    if not stats:
        return 0.0
    if stats.get("null_percentage") is not None:
        return stats["null_percentage"]
    if row_count and row_count > 0 and "null_count" in stats:
        return round((stats["null_count"] / row_count) * 100, 2)
    return 0.0


def _get_unique_pct(stats, row_count=0):
    if not stats:
        return 0.0
    if stats.get("unique_percentage") is not None:
        return stats["unique_percentage"]
    if row_count and row_count > 0 and "unique_count" in stats:
        return round((stats["unique_count"] / row_count) * 100, 2)
    return 0.0


def generate_markdown(schema_data: dict) -> str:
    """Generate a complete markdown data dictionary from schema data."""
    total_tables = len(schema_data)
    total_cols = sum(len(t.get("columns", {})) for t in schema_data.values())
    total_rows = sum(t.get("row_count", 0) for t in schema_data.values())
    avg_health = (
        sum(t.get("health_score", 100) for t in schema_data.values()) / total_tables
        if total_tables
        else 0
    )
    pii_cols = [
        f"{t}.{c}"
        for t, tm in schema_data.items()
        for c, cm in tm.get("columns", {}).items()
        if "PII" in (cm.get("tags") or [])
    ]

    md = "# Data Dictionary — SchemaDoc AI\n\n"
    md += "*Generated by SchemaDoc AI Pipeline*\n\n"
    md += "## Database Overview\n\n"
    md += f"| Metric | Value |\n|--------|-------|\n"
    md += f"| Tables | {total_tables} |\n"
    md += f"| Total Columns | {total_cols} |\n"
    md += f"| Total Rows | {total_rows:,} |\n"
    md += f"| Avg Health Score | {avg_health:.1f}/100 |\n"
    md += f"| PII Columns Detected | {len(pii_cols)} |\n\n"
    md += "---\n\n"

    for table, meta in schema_data.items():
        row_count = meta.get("row_count", 0)
        hs = meta.get("health_score", 100)
        md += f"## Table: `{table}`\n"
        md += f"**Rows:** {row_count:,} | **Health Score:** {hs}/100\n\n"
        fks = meta.get("foreign_keys", [])
        if fks:
            md += "**Foreign Keys:** "
            md += ", ".join(
                [
                    f"`{fk['column']}` → `{fk['referred_table']}.{fk['referred_column']}`"
                    for fk in fks
                ]
            )
            md += "\n\n"
        md += "| Column | Type | Null % | Unique % | Description | Tags |\n"
        md += "|--------|------|--------|----------|-------------|------|\n"
        for col_name, col_data in meta.get("columns", {}).items():
            stats = col_data.get("stats") or {}
            null_pct = _get_null_pct(stats, row_count)
            uniq_pct = _get_unique_pct(stats, row_count)
            tags_str = ", ".join(col_data.get("tags", []))
            desc = (col_data.get("description") or "—").replace("\n", " ").replace("|", "\\|")
            md += f"| `{col_name}` | `{col_data.get('original_type', 'N/A')}` | {null_pct}% | {uniq_pct}% | {desc} | {tags_str} |\n"
        md += "\n---\n\n"

    md += "*End of Data Dictionary*\n"
    return md


@router.get("/{run_id}/json")
@limiter.limit(READ_LIMIT)
async def export_json(request: Request, run_id: str):
    """Export enriched schema as JSON."""
    run = get_run(run_id, session_id=_sid(request))
    if not run:
        raise HTTPException(status_code=404, detail=f"Run '{run_id}' not found")
    schema = run.get("schema_enriched")
    if not schema:
        raise HTTPException(status_code=400, detail="No schema data available")

    content = json.dumps(schema, indent=2, cls=DecimalEncoder)
    return Response(
        content=content,
        media_type="application/json",
        headers={"Content-Disposition": f"attachment; filename=schema_{run_id}.json"},
    )


@router.get("/{run_id}/markdown")
@limiter.limit(READ_LIMIT)
async def export_markdown(request: Request, run_id: str):
    """Export enriched schema as Markdown data dictionary."""
    run = get_run(run_id, session_id=_sid(request))
    if not run:
        raise HTTPException(status_code=404, detail=f"Run '{run_id}' not found")
    schema = run.get("schema_enriched")
    if not schema:
        raise HTTPException(status_code=400, detail="No schema data available")

    md_content = generate_markdown(schema)
    return Response(
        content=md_content,
        media_type="text/markdown",
        headers={
            "Content-Disposition": f"attachment; filename=data_dictionary_{run_id}.md"
        },
    )


# ── Business Report Helpers ───────────────────────────────────────

def _compute_quality_issues(schema_data: dict) -> list[dict]:
    """Identify data quality problems from schema stats."""
    issues = []
    for table, meta in schema_data.items():
        hs = meta.get("health_score", 100)
        row_count = meta.get("row_count", 0)
        if hs < 60:
            issues.append({
                "severity": "critical",
                "table": table,
                "issue": f"Health score is critically low ({hs:.0f}/100)",
                "recommendation": "Investigate null columns, data integrity, and missing constraints.",
            })
        elif hs < 80:
            issues.append({
                "severity": "warning",
                "table": table,
                "issue": f"Health score below threshold ({hs:.0f}/100)",
                "recommendation": "Review columns with high null percentages and add validation rules.",
            })
        for col_name, col_data in meta.get("columns", {}).items():
            stats = col_data.get("stats") or {}
            null_pct = _get_null_pct(stats, row_count)
            if null_pct > 50:
                issues.append({
                    "severity": "critical",
                    "table": table,
                    "issue": f"Column `{col_name}` has {null_pct}% null values",
                    "recommendation": f"Consider making `{col_name}` NOT NULL or implementing a default value strategy.",
                })
            elif null_pct > 20:
                issues.append({
                    "severity": "warning",
                    "table": table,
                    "issue": f"Column `{col_name}` has {null_pct}% null values",
                    "recommendation": f"Review data pipeline to ensure `{col_name}` is being populated correctly.",
                })
            uniq_pct = _get_unique_pct(stats, row_count)
            tags = col_data.get("tags") or []
            if "PK" in tags and uniq_pct < 100 and row_count > 0:
                issues.append({
                    "severity": "critical",
                    "table": table,
                    "issue": f"Primary key `{col_name}` is not 100% unique ({uniq_pct}%)",
                    "recommendation": "Investigate duplicate primary key values — this indicates data corruption.",
                })
    return issues


def _compute_relationship_map(schema_data: dict) -> list[dict]:
    """Extract all FK relationships across the database."""
    rels = []
    for table, meta in schema_data.items():
        for fk in meta.get("foreign_keys", []):
            rels.append({
                "source_table": table,
                "source_column": fk["column"],
                "target_table": fk["referred_table"],
                "target_column": fk["referred_column"],
            })
    return rels


def _generate_ai_overview(schema_data: dict) -> dict:
    """Use Gemini to generate executive summary and recommendations."""
    try:
        # Build compact summary for prompt
        summary_lines = []
        total_rows = 0
        pii_cols = []
        for table, meta in schema_data.items():
            rc = meta.get("row_count", 0)
            total_rows += rc
            col_count = len(meta.get("columns", {}))
            hs = meta.get("health_score", 100)
            fk_count = len(meta.get("foreign_keys", []))
            summary_lines.append(f"- {table}: {rc:,} rows, {col_count} cols, health={hs:.0f}, {fk_count} FKs")
            for c, cm in meta.get("columns", {}).items():
                desc = cm.get("description") or ""
                if cm.get("potential_pii"):
                    pii_cols.append(f"{table}.{c}")
                if desc:
                    summary_lines.append(f"  └ {c}: {desc[:80]}")

        prompt = f"""You are a Senior Data Architect writing a business-ready database assessment report.

DATABASE SUMMARY:
{chr(10).join(summary_lines)}

Total tables: {len(schema_data)}, Total rows: {total_rows:,}, PII columns: {len(pii_cols)}

Generate a JSON object with these exact keys:
{{
  "executive_summary": "A 3-4 sentence executive summary of the database, its purpose, and overall health",
  "business_domain": "The detected business domain (e.g., E-Commerce, Music Retail, Manufacturing)",
  "key_findings": ["finding 1", "finding 2", "finding 3", "finding 4"],
  "recommendations": ["recommendation 1", "recommendation 2", "recommendation 3"],
  "data_governance_notes": "2-3 sentences about PII, compliance considerations",
  "overall_assessment": "One sentence: the overall data quality verdict"
}}

Output ONLY valid JSON. No markdown. No explanation."""

        llm = ChatGoogleGenerativeAI(
            model=settings.GEMINI_MODEL,
            google_api_key=settings.GOOGLE_API_KEY,
            temperature=0,
        )
        response = llm.invoke([
            SystemMessage(content="You output only valid JSON."),
            HumanMessage(content=prompt),
        ])
        import re
        text = response.content.strip()
        # Extract JSON from possible markdown fences
        json_match = re.search(r"\{.*\}", text, re.DOTALL)
        if json_match:
            return json.loads(json_match.group(0))
        return json.loads(text)
    except Exception as e:
        logger.warning(f"AI overview generation failed: {e}")
        return {
            "executive_summary": "AI overview generation was not available for this run.",
            "business_domain": "Unknown",
            "key_findings": [],
            "recommendations": [],
            "data_governance_notes": "Review PII columns manually.",
            "overall_assessment": "Manual review recommended.",
        }


def generate_business_report(schema_data: dict, run_id: str) -> dict:
    """Generate a comprehensive business-ready report document."""
    total_tables = len(schema_data)
    total_cols = sum(len(t.get("columns", {})) for t in schema_data.values())
    total_rows = sum(t.get("row_count", 0) for t in schema_data.values())
    avg_health = (
        sum(t.get("health_score", 100) for t in schema_data.values()) / total_tables
        if total_tables else 0
    )
    pii_cols = [
        f"{t}.{c}"
        for t, tm in schema_data.items()
        for c, cm in tm.get("columns", {}).items()
        if cm.get("potential_pii") or "PII" in (cm.get("tags") or [])
    ]
    fk_count = sum(len(t.get("foreign_keys", [])) for t in schema_data.values())

    # Get AI overview
    ai_overview = _generate_ai_overview(schema_data)

    # Compute quality issues
    quality_issues = _compute_quality_issues(schema_data)

    # Compute relationship map
    relationships = _compute_relationship_map(schema_data)

    # Per-table summaries
    table_summaries = []
    for table, meta in schema_data.items():
        rc = meta.get("row_count", 0)
        hs = meta.get("health_score", 100)
        cols = meta.get("columns", {})
        fks = meta.get("foreign_keys", [])
        desc = meta.get("description") or ""

        col_details = []
        for col_name, col_data in cols.items():
            stats = col_data.get("stats") or {}
            col_details.append({
                "name": col_name,
                "type": col_data.get("original_type", "N/A"),
                "description": col_data.get("description") or "",
                "business_logic": col_data.get("business_logic") or "",
                "tags": col_data.get("tags", []),
                "potential_pii": col_data.get("potential_pii", False),
                "null_percentage": _get_null_pct(stats, rc),
                "unique_percentage": _get_unique_pct(stats, rc),
                "sample_values": stats.get("sample_values", []),
                "min_value": stats.get("min_value"),
                "max_value": stats.get("max_value"),
                "mean_value": stats.get("mean_value"),
            })

        table_summaries.append({
            "table_name": table,
            "description": desc,
            "row_count": rc,
            "column_count": len(cols),
            "health_score": round(hs, 1),
            "foreign_keys": fks,
            "columns": col_details,
        })

    return {
        "report_metadata": {
            "generated_at": datetime.now(timezone.utc).isoformat(),
            "run_id": run_id,
            "generator": "SchemaDoc AI v2.0",
            "report_type": "Business-Ready Data Dictionary & Assessment",
        },
        "executive_overview": ai_overview,
        "database_statistics": {
            "total_tables": total_tables,
            "total_columns": total_cols,
            "total_rows": total_rows,
            "average_health_score": round(avg_health, 1),
            "pii_columns_detected": len(pii_cols),
            "pii_column_list": pii_cols,
            "foreign_key_count": fk_count,
        },
        "quality_issues": quality_issues,
        "relationships": relationships,
        "tables": table_summaries,
    }


def report_to_markdown(report: dict) -> str:
    """Convert a business report dict to a formatted markdown document."""
    md = "# SchemaDoc AI — Business Data Dictionary & Assessment Report\n\n"
    meta = report.get("report_metadata", {})
    md += f"*Generated by {meta.get('generator', 'SchemaDoc AI')} on {meta.get('generated_at', 'N/A')}*  \n"
    md += f"*Run ID: {meta.get('run_id', 'N/A')}*\n\n"
    md += "---\n\n"

    # Executive Overview
    overview = report.get("executive_overview", {})
    md += "## Executive Overview\n\n"
    md += f"**Business Domain:** {overview.get('business_domain', 'N/A')}\n\n"
    md += f"{overview.get('executive_summary', '')}\n\n"

    if overview.get("key_findings"):
        md += "### Key Findings\n\n"
        for f in overview["key_findings"]:
            md += f"- {f}\n"
        md += "\n"

    if overview.get("recommendations"):
        md += "### Recommendations\n\n"
        for r in overview["recommendations"]:
            md += f"- {r}\n"
        md += "\n"

    if overview.get("data_governance_notes"):
        md += "### Data Governance Notes\n\n"
        md += f"{overview['data_governance_notes']}\n\n"

    md += f"**Overall Assessment:** {overview.get('overall_assessment', 'N/A')}\n\n"
    md += "---\n\n"

    # Database Statistics
    stats = report.get("database_statistics", {})
    md += "## Database Statistics\n\n"
    md += "| Metric | Value |\n|--------|-------|\n"
    md += f"| Total Tables | {stats.get('total_tables', 0)} |\n"
    md += f"| Total Columns | {stats.get('total_columns', 0)} |\n"
    md += f"| Total Rows | {stats.get('total_rows', 0):,} |\n"
    md += f"| Average Health Score | {stats.get('average_health_score', 0)}/100 |\n"
    md += f"| PII Columns Detected | {stats.get('pii_columns_detected', 0)} |\n"
    md += f"| Foreign Key Relationships | {stats.get('foreign_key_count', 0)} |\n\n"

    if stats.get("pii_column_list"):
        md += "**PII Columns:** " + ", ".join(f"`{c}`" for c in stats["pii_column_list"]) + "\n\n"

    md += "---\n\n"

    # Quality Issues
    issues = report.get("quality_issues", [])
    if issues:
        md += "## Data Quality Issues\n\n"
        critical = [i for i in issues if i["severity"] == "critical"]
        warnings = [i for i in issues if i["severity"] == "warning"]
        if critical:
            md += f"### Critical ({len(critical)})\n\n"
            for issue in critical:
                md += f"- **{issue['table']}**: {issue['issue']}  \n"
                md += f"  *Recommendation:* {issue['recommendation']}\n"
            md += "\n"
        if warnings:
            md += f"### Warnings ({len(warnings)})\n\n"
            for issue in warnings:
                md += f"- **{issue['table']}**: {issue['issue']}  \n"
                md += f"  *Recommendation:* {issue['recommendation']}\n"
            md += "\n"
        md += "---\n\n"

    # Relationships
    rels = report.get("relationships", [])
    if rels:
        md += "## Table Relationships\n\n"
        md += "| Source Table | Column | → | Target Table | Column |\n"
        md += "|-------------|--------|---|-------------|--------|\n"
        for r in rels:
            md += f"| `{r['source_table']}` | `{r['source_column']}` | → | `{r['target_table']}` | `{r['target_column']}` |\n"
        md += "\n---\n\n"

    # Table Details
    md += "## Table Documentation\n\n"
    for table in report.get("tables", []):
        hs = table["health_score"]
        md += f"### `{table['table_name']}`\n\n"
        if table.get("description"):
            md += f"_{table['description']}_\n\n"
        md += f"**Rows:** {table['row_count']:,} | **Columns:** {table['column_count']} | **Health:** {hs}/100\n\n"
        fks = table.get("foreign_keys", [])
        if fks:
            md += "**Foreign Keys:** " + ", ".join(
                f"`{fk['column']}` → `{fk['referred_table']}.{fk['referred_column']}`" for fk in fks
            ) + "\n\n"

        md += "| Column | Type | Null% | Unique% | PII | Description |\n"
        md += "|--------|------|-------|---------|-----|-------------|\n"
        for col in table.get("columns", []):
            pii = "⚠️" if col.get("potential_pii") else ""
            desc = (col.get("description") or "—").replace("\n", " ").replace("|", "\\|")[:100]
            md += f"| `{col['name']}` | `{col['type']}` | {col['null_percentage']}% | {col['unique_percentage']}% | {pii} | {desc} |\n"
        md += "\n---\n\n"

    md += "*End of Report — SchemaDoc AI*\n"
    return md


# ── Report API Endpoints ──────────────────────────────────────────

@router.get("/{run_id}/report")
@limiter.limit(EXPORT_REPORT_LIMIT)
async def export_report_json(request: Request, run_id: str):
    """Generate and return AI-enhanced business report as JSON."""
    sid = _sid(request)
    run = get_run(run_id, session_id=sid)
    if not run:
        raise HTTPException(status_code=404, detail=f"Run '{run_id}' not found")
    schema = run.get("schema_enriched")
    if not schema:
        raise HTTPException(status_code=400, detail="No schema data available")

    # Serve from cache if available
    cache_key = f"{sid}:{run_id}"
    if cache_key in _report_cache:
        return JSONResponse(content=_report_cache[cache_key])

    report = generate_business_report(schema, run_id)
    clean = json.loads(json.dumps(report, cls=DecimalEncoder))
    _report_cache[cache_key] = clean
    return JSONResponse(content=clean)


@router.get("/{run_id}/report/markdown")
@limiter.limit(EXPORT_REPORT_LIMIT)
async def export_report_markdown(request: Request, run_id: str):
    """Generate and return AI-enhanced business report as Markdown."""
    sid = _sid(request)
    run = get_run(run_id, session_id=sid)
    if not run:
        raise HTTPException(status_code=404, detail=f"Run '{run_id}' not found")
    schema = run.get("schema_enriched")
    if not schema:
        raise HTTPException(status_code=400, detail="No schema data available")

    # Reuse cached report if available
    cache_key = f"{sid}:{run_id}"
    if cache_key in _report_cache:
        report = _report_cache[cache_key]
    else:
        report = generate_business_report(schema, run_id)
        _report_cache[cache_key] = json.loads(json.dumps(report, cls=DecimalEncoder))

    md_content = report_to_markdown(report)
    return Response(
        content=md_content,
        media_type="text/markdown",
        headers={
            "Content-Disposition": f"attachment; filename=business_report_{run_id}.md"
        },
    )
